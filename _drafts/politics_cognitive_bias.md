---
layout: post
title: "Cognitive Bias, Politics and Fact-Checking"
date: 2026-01-09
tags: [politics, psychology]
---

A [woman has been killed](https://www.bbc.co.uk/news/articles/c93vny222geo) in the US by ICE officers and the BBC are right to call
out that this is really highlighting the "two starkly opposed Americas". The
last thing I want to do is actually wade into the debate surrounding this
specific incident.

But I do want to talk about congnitive bias and how clear it is in a case like
this. There are multiple videos of the incident from multiple different angles.
If **ever** there was going to be a chance to have an *'objective'* view,
surely this would be it?

And yet intelligent individuals apparently watch the same footage and, broadly
along the lines of their pre-existing biases, form completely different
opinions. Of course, this isn't a unique case: our pre-existing biases cause us
to view and interpret evidence in different ways to people with different
biases. Indeed, that's one of the stated benefits of a diverse team working on
a project (where I'm using *'diverse'* to mean a team where people given the
same input come up with different output - not whatever the current overloaded
term has come to mean).

## Fact Checking

A couple of years ago there was a fairly big hoo-hah about 'fact-checking' - a
persistent fight against 'misinformation'. I always thought (and still do) that
this was misguided. The reason is basically sketched out in the case above.
Same video. Same set of facts. Two completely different interpretations. What
counts as misinformation in this case?

Do we have to wait for a judge to decide on the case before determining which
interpretation is misinformation? Do we let each tech company form its own
panel using its own methods and have them vote on which interpretation they
like? Do we take the Twitter/X like approach and crowdsource which
interpretation is 'correct' based on voting? 

None of these seem great to me. The world is messy and two people can interpret
the same raw material into two completely different ways. Generally we're OK
with that, but when it comes to potential harm and politics in particular, we
only want people to be consuming ***the truth***.

But what is the truth? Do you only allow people to make purely factual claims?
Do you only allow people to state an opinion if it's clearly prefaced with "in
my opinion". And what about connected statements that imply (but dont' claim)
causation?

"We send the EU £350 million a week, let's fund our NHS instead."

One of the slogans of Vote Leave during the EU referendum. What do you do with that? If you're an impartial fact-checker?

Firstly, do we send the EU £350 million a week? I think the answer is "not
really, but if you squint you could make the claim". And could we fund the NHS
instead? Sure.

Some people took this to mean that if we left the EU we would instead give the
NHS an additional £350 million a week. Sure, that's a fine interpretation and
is probably how it was designed to be read. But crucially (maybe), that isn't
what it says.

How would you possibly fact check this? I'd argue that you couldn't - it isn't
misinformation. There's a 'fact' (an aggressive interpretation of numbers) and
then a statement of priorities. In the past we relied upong 'trusted
institutions' to handle this for us - but in recent years their shortcomings
have become all too apparent.

## Trusted Institutions

Historically, if the BBC or WHO or other three-letter agencies made a
statement, you'd generally assume it was correct. In recent years though, under increasing scrutiny, it's become apparant that those institutions are
staffed by people. People who have their own biases.

What's more, they're staffed by pretty non-nationally/globally-representative
samples of people. Generally to get a position in one of these institutions
(particularly a senior position) you have to go to University, then live in a
big city, and work and surround yourself with other 'intellectuals' who also
went to University and live in a big city.

No worries...let's get tech companies to do it?

The problem, of course, is exactly the same - tech companies are
diverse in some measures, but in others, they're woefully homogenous.

This is one of the reasons I was shocked to see so many people accept/suggest
that tech companies should be the ones responsible for
marking/deleting/flagging things that they considered to be 'misinformation'.
The truth isn't always clear, and is often subject to change given new
information. To allow an ideological hive mind to determine what 'the truth' is
was never going to work.

## Amplification

One of the arguments about what makes tech companies different is the
**amplification** of potentially false information. It used to be that my
'reach' was small, unless I could persuade a journalist or politician to share.
There were gatekeepers.

Now, social media companies are the gatekeepers. Except they aren't.

Social media companies don't decide what content to boost. Algorithms do. But
that's a cop out, because algorithms don't *'decide'* for any real definition
of the word decide. Algorithms ultimatley have to decide what to amplify and
they do so based on the choices that the people who programmed them make. Now
the problem is, we'd like people to value quality educational and balanced
content. But people act like people, and it turns out that people either like
content that angers them, or content that confirms their existing view of the
world.

So tech companies have a choice - is their job to 'benevolently' attempt to
improve us, forcing us to view content that we don't want because it'll broaden
our worldview? Or should they just give us what we want to see,
consequences be damned?

Tech companies would argue that if they don't give us what we want, we'll go to
a different platform that will. Media companies who have contractual
obligations to ensure a certain standard of quality on their platforms would
argue otherwise.

## One Decision Leads to Another

Let's say they do - if Facebook sees you entering a rabbit-hole, it's their
responsibility to pull you out of it. What counts as a rabbit-hole? If I spend
hours and hours delving deeper into the world of personal finance, presumably
that's OK, and showing me personal finance content is ideal.

What about fan-fiction? Religious material? Exercise content? Crime-scene
investigations? Historic crime-scene investigations? War re-enactments? Medical
investigations? Parenting advice?

Ineviatably, all of these *might* be fine and positive. And all of them *might*
veer into the unacceptable. Content explaining how you might cut your calories
might be used sensibly, or they might lead a young person towards anorexia.
Is questioning mask use during a pandemic a valid public health question, or is
it undermining public health response? Who should decide?


The panel of 'experts'? A nationally representative sample? The legal system?
AI?

## Conclusion

I believe that ultimately, the answer is "the legal system". I think that tech
companies acting as 'benevolent' overseers of what is considered 'true' or
'acceptable' is both morally repugnant, and technically fraught. I don't
believe tech want this responsibility, and I don't think they're qualified to
exercise it.

The legal system, however, has been exercising that kind of power for as long
as it has existed. Many would argue that the legal system is too slow and isn't
able to efficiently regulate tech companies. The solution is not to move the
power to regulate our online lives to (largely) American entitites, but instead
to reform the judiciary to better handle today's largely-online world.
